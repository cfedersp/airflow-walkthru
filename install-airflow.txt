Install python 3.9.x
Later versions may not match existing constraints

# Define and Create a dir for Airflow to store its Configuration, development DAGs, DB, and Users.
# Should be outside this repo.

export AIRFLOW_HOME=$HOME/opt/airflow
mkdir -p $AIRFLOW_HOME

# Optionally, which global python and pip, upgrade pip and Check whats available
python3 -m pip install --upgrade pip
python3 -m pip index versions apache-airflow

# Set Airflow version and python env name
AIRFLOW_VERSION="3.0.1"
VENV_NAME="aaf${AIRFLOW_VERSION}"

# Create an isolated python environment for your Airflow project:
python3 -m venv ${VENV_NAME}

# Have 2 Terminals open with the python environment active
echo 'export no_proxy=*' >> ${VENV_NAME}/bin/activate
source ${VENV_NAME}/bin/activate



# Airflow requirements depend on the version of python. The Constraint Set dictates the package versions for all required libs.

CONSTRAINT_SET=$(python --version | egrep -o '[0-9]+.[0-9]+')

# skip celery for now
pip install "apache-airflow[amazon,aiobotocore]==${AIRFLOW_VERSION}" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${CONSTRAINT_SET}.txt"

# Install Optional dependencies:
pip install graphviz flask_appbuilder apache-airflow-providers-common-messaging apache-airflow-providers-jdbc
# pip install apache-airflow-providers-mysql
pip install apache-airflow-providers-oracle

# pip install apache-airflow-providers-redis
# pip install apache-airflow-providers-amazon


# Check required dependencies are in your environment
pip show apache-airflow-providers-common-messaging
find ${VENV_NAME} -name '*.py' | xargs fgrep MessageQueueTrigger  
pip show apache-airflow-providers-redis
find ${VENV_NAME} -name '*.py' | xargs fgrep RedisMessageQueueTrigger 
pip show JPype1
pip show jaydebeapi

# Write the default config to AIRFLOW_HOME:
airflow config list --defaults > $AIRFLOW_HOME/airflow.cfg

# Update airflow.cfg:
'jwt_secret' property must have a value
Uncomment 'simple_auth_manager_users' so that admin:admin is defined (username:role)

# Setup JDBC Driver (check if providers.jdbc exists already)
echo "[providers.jdbc]" >> $AIRFLOW_HOME/airflow.cfg
echo "allow_driver_path_in_extra = True" >> $AIRFLOW_HOME/airflow.cfg
echo "allow_driver_class_in_extra = True" >> $AIRFLOW_HOME/airflow.cfg
mkdir $AIRFLOW_HOME/lib
cp Downloads/ojdbc11-23.8.0.25.04.jar $AIRFLOW_HOME/lib

# Create Connections using env vars (Can only be done after the db has been initialized)
airflow connections create-default-connections
airflow connections add 'fs_default' --conn-type 'fs'
airflow connections add 'oracle' --conn-description 'Oracle DB' --conn-uri 'jdbc:oracle:thin:cfederspiel/password@//d1entp-scan.res.prod.global:1521/D1SFRC_BATCH' --conn-extra '{"driver_path":"/Users/chafeder/opt/airflow/lib/ojdbc11-23.8.0.25.04.jar","driver_class":"oracle.jdbc.OracleDriver"}'
# jdbc:mysql://
airflow connections add 'mysql' --conn-type 'jdbc' --conn-description 'Local MySQL' --conn-host 'localhost' --conn-port 3306 --conn-login root --conn-password password --conn-extra '{"driver_path":"/Users/chafeder/opt/airflow/lib/mysql-connector-j-9.3.0.jar","driver_class":"com.mysql.cj.jdbc.Driver"}'
airflow connections add 'mysql' --conn-type 'jdbc:mysql' --conn-description 'EC2 MySQL' --conn-host 'localhost' --conn-port 3306 --conn-schema searchforce --conn-login root --conn-password password --conn-extra '{"driver_path":"/home/ec2-user/opt/airflow/lib/mysql-connector-j-9.3.0.jar","driver_class":"com.mysql.cj.jdbc.Driver"}'
airflow connections add 'mysql' --conn-description 'MySQL DB' --conn-uri 'mysql+mysqlconnector://root:password@localhost:3306/searchforce' --conn-extra '{"driver_path":"/home/ec2-user/opt/airflow/lib/mysql-connector-j-9.3.0.jar","driver_class":"com.mysql.cj.jdbc.Driver"}'
airflow connections add 'mysql' --conn-description 'MySQL DB' --conn-uri 'mysql+mysqlconnector://localhost:3306/searchforce?user=root&password=password' --conn-extra '{"driver_path":"/home/ec2-user/opt/airflow/lib/mysql-connector-j-8.4.0.jar","driver_class":"com.mysql.cj.jdbc.Driver"}'
airflow connections add 'mysql' --conn-type 'jdbc' --conn-description 'EC2 MySQL' --conn-host 'localhost' --conn-port 3306 --conn-schema searchforce --conn-login root --conn-password password --conn-extra '{"driver_path":"/home/ec2-user/opt/airflow/lib/mysql-connector-j-8.4.0.jar","driver_class":"com.mysql.cj.jdbc.Driver"}'

airflow connections add 'redis' --conn-type 'redis' --conn-description 'Staging Redis' --conn-host 'redis-host.company.com' --conn-port 6383 
# --conn-extra 'db=0'

# networking, if necessary (didn't work)
sudo yum install iptables-services
sudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080

# Install Docker 
sudo dnf install docker
sudo usermod -aG docker ec2-user
exit

# Create mysql db
Start podman machine, wait for it to start
podman system connection list
podman run -dt --name airflow-mysql -e MYSQL_ROOT_PASSWORD=password -p 3306:3306 mysql:latest
podman exec -it airflow-mysql mysql --user=root -p
create database searchforce;
use searchforce;
CREATE TABLE REVIEW_QUEUE (SID int PRIMARY KEY, REVIEW_TYPE int, DESCRIPTION VARCHAR(200), STATUS varchar(20), AIRFLOW_INGESTED_DATE DATE, CREATED_DATE DATE);
INSERT INTO searchforce.REVIEW_QUEUE (SID, REVIEW_TYPE, DESCRIPTION, STATUS, CREATED_DATE) values (1111, 222222, 'Ready Campaign1', 'READY', NOW());

# Create Oracle DB
CREATE TABLE cfederspiel.REVIEW_QUEUE (SID int PRIMARY KEY, REVIEW_TYPE int, DESCRIPTION VARCHAR(200), STATUS varchar(20), AIRFLOW_INGESTED_DATE DATE, CREATED_DATE DATE);
INSERT INTO cfederspiel.REVIEW_QUEUE (SID, REVIEW_TYPE, DESCRIPTION, STATUS, CREATED_DATE) values (1111, 222222, 'Ready Campaign1', 'READY', SYSDATE)

# Post some data
aws configure sso
aws configure list-profiles
export AWS_DEFAULT_PROFILE=identity-center-user-profile
aws sqs create-queue --queue-name CJF_EPSILON_REVIEW_QUEUE --region us-west-2  --profile identity-center-user-profile
# DO NOT MODIFY the aws_default connection, except to optionally add aws_region! The aws python sdk (boto3) knows where to find your credentials.
# "QueueUrl": "https://sqs.us-west-2.amazonaws.com/339713066603/CJF_EPSILON_REVIEW_QUEUE"

# Show the current Executor:
airflow config get-value core executor
airflow config get-value providers.jdbc allow_driver_path_in_extra

# standalone includes the triggerer process
airflow standalone

The admin pw is generated on first startup
Visit http:/localhost:8080
Print the admin pw and paste into login screen
cat $AIRFLOW_HOME/simple_auth_manager_passwords.json.generated

# CHECK ALL COMPONENTS ARE GREEN IN UI: MetaDatabase, Scheduler, Triggerer, DAG Processor
# Click 'Admin' -> Providers, ensure amazon and common-messaging are included.

pip freeze > requirements.txt
next time you activate:
source ${VENV_NAME}/bin/activate
pip install -r requirements.txt

# Troubleshooting Config:
python -m  airflow config get-value providers.jdbc allow_driver_path_in_extra
View config in Admin Console:
Add expose_config = True under [webserver]

Pause a faulty DAG
airflow dags pause review_queue_pipeline

Delete DAG metadata from DB:
airflow dags delete ProcessFileForDay

Reference:
https://www.astronomer.io/docs/learn/airflow-event-driven-scheduling/
Event-driven scheduling in Airflow 3.0 is supported for Amazon SQS with support for other message queues planned for future releases.
Queue Providers:
SQS:
Kafka: https://airflow.apache.org/docs/apache-airflow-providers-apache-kafka/stable/message-queues/index.html
Redis:
Custom: https://airflow.apache.org/docs/apache-airflow-providers-common-messaging/stable/_api/airflow/providers/common/messaging/providers/base_provider/index.html#classes