Install python 3.9.x
Later versions may not match existing constraints

# Define and Create a dir for Airflow to store its Configuration, development DAGs, DB, and Users.
# Should be outside this repo.
export AIRFLOW_HOME=$HOME/opt/airflow
mkdir -p $AIRFLOW_HOME

# Create an isolated python environment for your Airflow project:
python3 -m venv local

# Have 2 Terminals open with the python environment active
source local/bin/activate

# Airflow requirements depend on the version of python. The Constraint Set dictates the package versions for all required libs.
CONSTRAINT_SET=$(python3 --version | egrep -o '\d+.\d+')

local/bin/python3 -m pip install "apache-airflow[celery]==3.0.1" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.1/constraints-${CONSTRAINT_SET}.txt"
or (not tested)
local/bin/python3 -m pip install https://files.pythonhosted.org/packages/68/46/59af3aa40b2f26e55ee2208c9c7076c406aadc0d6cbb11c220659987d0e8/apache_airflow-3.0.1-py3-none-any.whl[celery] --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.2/constraints-${CONSTRAINT_SET}.txt"

# Install Optional dependencies:
local/bin/python3 -m pip install graphviz
local/bin/python3 -m pip install apache-airflow-providers-common-messaging
local/bin/python3 -m pip install apache-airflow-providers-redis

# Check required dependencies are in your environment
local/bin/python3 -m pip show apache-airflow-providers-common-messaging
find local -name '*.py' | xargs fgrep MessageQueueTrigger  
local/bin/python3 -m pip show apache-airflow-providers-redis
find local -name '*.py' | xargs fgrep RedisMessageQueueTrigger 

# Write the default config to AIRFLOW_HOME:
python3 -m airflow config list --defaults > $AIRFLOW_HOME/airflow.cfg

# Update airflow.cfg:
'jwt_secret' property must have a value
Uncomment 'simple_auth_manager_users' so that admin:admin is defined (username:role)

# Create Connections using env vars
airflow connections add 'fs_default' --conn-type 'fs'
airflow connections add 'redis' --conn-type 'redis' --conn-description 'Staging Redis' --conn-host 'redis-host.company.com' --conn-port 6383 
# --conn-extra 'db=0'

# Show the current Executor:
airflow config get-value core executor

local/bin/python3 -m airflow standalone
The admin pw is generated on first startup
Visit http:/localhost:8080
Print the admin pw:
cat $AIRFLOW_HOME/simple_auth_manager_passwords.json.generated

pip3 freeze > requirements.txt
next time you activate:
pip install -r requirements.txt


Reference:
https://www.astronomer.io/docs/learn/airflow-event-driven-scheduling/
Event-driven scheduling in Airflow 3.0 is supported for Amazon SQS with support for other message queues planned for future releases.
Queue Providers:
SQS:
Kafka: https://airflow.apache.org/docs/apache-airflow-providers-apache-kafka/stable/message-queues/index.html
Redis:
Custom: https://airflow.apache.org/docs/apache-airflow-providers-common-messaging/stable/_api/airflow/providers/common/messaging/providers/base_provider/index.html#classes